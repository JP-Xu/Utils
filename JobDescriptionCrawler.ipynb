{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60094ad6-ef3b-4933-afc2-7f159a073e4a",
   "metadata": {},
   "source": [
    "---\n",
    "## Code Introduction\n",
    "\n",
    "### Overview\n",
    "This code performs the following tasks:\n",
    "\n",
    "1. **Extracts Job Links**: Scrapes job links from LinkedIn and Glassdoor.\n",
    "2. **Web Crawling**: Utilizes Selenium to crawl and gather information from the extracted links.\n",
    "3. **Job Description Extraction**: Leverages ChatGPT 3.5 API to extract job descriptions.\n",
    "4. **Data Sorting and Saving**: Sorts the collected data and saves it to a spreadsheet.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e0d38-2657-44b3-86cf-a54c63d1c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exchangelib import DELEGATE, Account, Credentials, Folder\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import pandas as pd\n",
    "import time\n",
    "import openpyxl\n",
    "from datetime import date\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import regex as re\n",
    "# Initialize OpenAI API key\n",
    "openai.api_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef11df-8afd-46bc-b021-b1a12fd0905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_details_from_text(text):\n",
    "    # Initialize OpenAI API key (use environment variable or directly input your API key)\n",
    "\n",
    "    # Define the prompt to extract information\n",
    "    prompt = f\"From the following job summary, use bullet points if possible, extract details like 'Job Title', 'Company Name', 'Job Link', 'Job Location', 'Salary', 'Number of Applicants', 'Posted Days Ago', 'Job Diploma Requirement', 'Job Years of Experience Requirement', 'Job Description':\\n{text}\\nDetails:\"\n",
    "    \n",
    "    # Make the API call with GPT-3.5 Turbo\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in extracting job details.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get the generated text\n",
    "    details = response['choices'][0]['message']['content'].strip()\n",
    "    \n",
    "    return details\n",
    "\n",
    "\n",
    "def job_cralwer(url, plat='Linkedin'):\n",
    "    # Set a timeout of 10 seconds\n",
    "    driver.set_page_load_timeout(10)\n",
    "    \n",
    "    # Load a URL\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except TimeoutException:\n",
    "        print(\"Timed out waiting for the page to load\")\n",
    "        return None\n",
    "        \n",
    "    job_info = \"\"\n",
    "    if plat=='Linkedin':\n",
    "        classes = ['jobs-unified-top-card__primary-description','jobs-company__box', 'jobs-description__container']\n",
    "        # Expand job description\n",
    "        time.sleep(3)\n",
    "        driver.find_element(By.XPATH, '//span[text()=\"See more\"]').click()\n",
    "        # Loop through each class name to find the elements, get text, and append it to joined_text\n",
    "        for class_name in classes:\n",
    "            xpath_expression = f\"//*[contains(@class, '{class_name}')]\"\n",
    "            elements = driver.find_elements(By.XPATH, xpath_expression)\n",
    "            \n",
    "            for element in elements:\n",
    "                job_info += element.text + \" \"\n",
    "        return job_info\n",
    "    elif plat == 'Glassdoor':\n",
    "        job_info_expression = f'//*[@id=\"JobDescriptionContainer\"]'\n",
    "        job_header_expression = f'//div[@class=\"d-flex\"]'\n",
    "        for xpath_exp in [job_info_expression, job_header_expression]:\n",
    "            elements = driver.find_elements(By.XPATH, xpath_exp)\n",
    "        \n",
    "            for element in elements:\n",
    "                job_info += element.text + \" \"\n",
    "            \n",
    "        return job_info\n",
    "\n",
    "\n",
    "\n",
    "def parse_job_string(job_str):\n",
    "    job_dict = {}\n",
    "    description_started = False\n",
    "    description_list = []\n",
    "    others_list = []\n",
    "    others_started = False\n",
    "\n",
    "    for line in job_str.split('\\n'):\n",
    "        # Handle the 'Job Description' section\n",
    "        if description_started:\n",
    "            if re.match(r'\\s+-', line):  # Bullet points in description\n",
    "                description_list.append(line.strip('- '))\n",
    "                continue\n",
    "            else:  # Switch to 'Others' section\n",
    "                description_started = False\n",
    "                others_started = True\n",
    "\n",
    "        # Handle the 'Others' section\n",
    "        if others_started:\n",
    "            others_list.append(line.strip('- '))\n",
    "            continue\n",
    "\n",
    "        # Handle other key-value pairs\n",
    "        if ': ' in line or line.strip() == \"- Job Description:\":\n",
    "            key, value = line.split(': ', 1) if ': ' in line else (line.strip('- :'), None)\n",
    "            key = key.lstrip('- ').rstrip()  # Remove leading dashes and spaces\n",
    "            if key == 'Job Description':\n",
    "                description_started = True\n",
    "            job_dict[key] = value if value else []\n",
    "\n",
    "    job_dict['Job Description'] = \"- \"+\"\\n- \".join(description_list)\n",
    "    job_dict['Others'] = None if not others_list else '\\n'.join(others_list)\n",
    "    job_dict['Queried Date'] = date.today()\n",
    "    return job_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f32de0-7bcf-4ffa-bfd8-bcd23f29d8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exchangelib import DELEGATE, Account, Credentials, Configuration\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from exchangelib import Credentials, Account, Configuration\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define your credentials\n",
    "credentials = Credentials(username='', password='')\n",
    "config = Configuration(server='outlook.office365.com', credentials=credentials)\n",
    "\n",
    "# Connect to the account\n",
    "account = Account(\n",
    "    primary_smtp_address='', \n",
    "    config=config,\n",
    "    autodiscover=False, \n",
    "    access_type=DELEGATE\n",
    ")\n",
    "\n",
    "# Access the inbox\n",
    "inbox = account.inbox\n",
    "\n",
    "\n",
    "# Delete after using\n",
    "# Get the most recent three emails\n",
    "recent_emails = account.inbox.all().order_by('-datetime_received')[:3]\n",
    "\n",
    "for item in recent_emails:\n",
    "    # Assuming 'body' contains the email body in HTML format\n",
    "    email_body = item.body\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(email_body, 'html.parser')\n",
    "\n",
    "    # Extract all href attributes\n",
    "    all_hrefs = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "account.protocol.close()\n",
    "\n",
    "# Your timezone\n",
    "local_tz = pytz.timezone('America/New_York')\n",
    "\n",
    "# Date and time for filtering emails received today, made timezone-aware\n",
    "start_time = local_tz.localize(datetime.now().replace(hour=0, minute=0, second=0, microsecond=0))\n",
    "end_time = local_tz.localize(datetime.now().replace(hour=23, minute=59, second=59, microsecond=999999))\n",
    "\n",
    "job_urls = {\n",
    "    'Linkedin' : [],\n",
    "    'Glassdoor' : []\n",
    "}\n",
    "clean_job_urls = job_urls.copy()\n",
    "linkedin_job_urls_list = []\n",
    "\n",
    "# Loop through unread emails in the inbox received today\n",
    "for item in inbox.filter(is_read=True, datetime_received__range=(start_time, end_time)):\n",
    "    # Assuming 'body' contains the email body in HTML format\n",
    "    email_body = item.body\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(email_body, 'html.parser')\n",
    "\n",
    "    # Extract all href attributes\n",
    "    all_hrefs = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    \n",
    "    # Filter out LinkedIn job URLs\n",
    "    linkedin_job_urls = [url for url in all_hrefs if 'jobs/view/' in url]\n",
    "    gd_job_urls = [html.unescape(url) for url in all_hrefs if 'jobListing' in url]\n",
    "    \n",
    "    if linkedin_job_urls:\n",
    "        # Append these URLs to the list\n",
    "        job_urls['Linkedin'].extend(linkedin_job_urls)\n",
    "    if gd_job_urls:\n",
    "        job_urls['Glassdoor'].extend(gd_job_urls)\n",
    "        \n",
    "job_urls['Linkedin'] = list(set(job_urls['Linkedin']))\n",
    "clean_job_urls['Linkedin'] = [url.split(\"?\")[0] for url in job_urls['Linkedin']]\n",
    "clean_job_urls['Glassdoor'] = job_urls['Glassdoor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b54321-010f-48ba-8432-19dd3498bc3c",
   "metadata": {},
   "source": [
    "## Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bce877a-245e-46f0-87ec-786e47152c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "options = webdriver.EdgeOptions()\n",
    "options.add_argument(\"Application Support/Microsoft Edge\")\n",
    "options.add_argument(\"--profile-directory=Profile 1\")\n",
    "# Initialize the WebDriver for Edge\n",
    "driver = webdriver.Edge(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ac19e-b180-4d03-9314-47369d3b2dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.linkedin.com/')\n",
    "time.sleep(1.2)\n",
    "driver.find_element(By.XPATH,'//input[@id=\"session_key\"]').send_keys('')\n",
    "driver.find_element(By.XPATH,'//input[@id=\"session_password\"]').send_keys('')\n",
    "driver.find_element(By.XPATH,'//button[@data-id=\"sign-in-form__submit-btn\"]').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb78d87-545b-4714-87be-650e685e4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('JobHost.xlsx')\n",
    "for url in clean_job_urls['Linkedin']:\n",
    "    # Check if the URL already exists in the DataFrame\n",
    "    if url not in df['Job Link'].values:\n",
    "        print(url+' Not Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212bf881-7b2d-48d0-9ae0-8962b833be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in clean_job_urls['Linkedin']:\n",
    "    # Check if the URL already exists in the DataFrame\n",
    "    if url not in df['Job Link'].values:\n",
    "        # If it doesn't exist, scrape the job details\n",
    "        job_description = job_cralwer(url)\n",
    "        \n",
    "        # Add the URL to the job description\n",
    "        job_description = f'- Link:{url}\\n' + job_description\n",
    "        gpt_returns = extract_job_details_from_text(job_description)\n",
    "        # Add the job details to the DataFrame\n",
    "        job_details_json = parse_job_string(gpt_returns)\n",
    "        df_new = pd.DataFrame([job_details_json])\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        time.sleep(1)\n",
    "        driver.get('https://www.linkedin.com/feed/')\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ff9ef-f1b0-4939-a6d2-bdb2500dfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('JobHost.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f8258-1740-4fc7-ab4c-75f937edd675",
   "metadata": {},
   "source": [
    "## Glassdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5bbe29-64c2-47b4-9734-389b2ddd7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('GlassdoorJobPool.xlsx')\n",
    "new_urls = [ url for url in clean_job_urls['Glassdoor'] if url not in df['Job Link'].values]\n",
    "print(len(new_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093a0308-89b3-4384-bfd0-ff7a597c996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "options = webdriver.EdgeOptions()\n",
    "options.add_argument(\"Application Support/Microsoft Edge\")\n",
    "options.add_argument(\"--profile-directory=Profile 1\")\n",
    "# Initialize the WebDriver for Edge\n",
    "driver = webdriver.Edge(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081aac75-580f-4d48-b205-66361dbeb99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in new_urls[::3]:\n",
    "    # Check if the URL already exists in the DataFrame\n",
    "    if url not in df['Job Link'].values:\n",
    "        # If it doesn't exist, scrape the job details\n",
    "        job_description = job_cralwer(url, plat='Glassdoor')\n",
    "        \n",
    "        # Add the URL to the job description\n",
    "        job_description = f'- Link:{url}\\n' + job_description\n",
    "        gpt_returns = extract_job_details_from_text(job_description)\n",
    "        # Add the job details to the DataFrame\n",
    "        job_details_json = parse_job_string(gpt_returns)\n",
    "        df_new = pd.DataFrame([job_details_json])\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        time.sleep(1)\n",
    "        driver.get('https://google.com')\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c90eb7-79f5-405e-9af2-76ad4c7b2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('GlassdoorJobPool.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
